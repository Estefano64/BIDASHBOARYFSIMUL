"""
M√≥dulo de Web Scraping para noticias econ√≥micas de Per√∫ y el mundo
Sitios objetivo: La Gesti√≥n, La Rep√∫blica, El Comercio, Kitco, Mining.com
"""
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import time
import warnings
warnings.filterwarnings('ignore')

class WebScraperNoticias:
    """Scraper de noticias sobre oro y econom√≠a"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.timeout = 10
    
    def scrape_gestion(self, max_noticias=20):
        """
        Scrapear noticias de La Gesti√≥n (Per√∫)
        URL: https://gestion.pe/noticias/oro/
        """
        noticias = []
        
        try:
            print("  üì∞ Scrapeando La Gesti√≥n...")
            
            # URLs de b√∫squeda
            urls = [
                'https://gestion.pe/noticias/oro/',
                'https://gestion.pe/noticias/precio-oro/',
                'https://gestion.pe/economia/'
            ]
            
            for url in urls[:1]:  # Por ahora solo 1 URL para no saturar
                try:
                    response = requests.get(url, headers=self.headers, timeout=self.timeout)
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # Buscar art√≠culos (la estructura puede cambiar)
                    articulos = soup.find_all('div', class_='story-item', limit=max_noticias)
                    
                    if not articulos:
                        # Intentar otra estructura com√∫n
                        articulos = soup.find_all('article', limit=max_noticias)
                    
                    for articulo in articulos[:max_noticias]:
                        try:
                            # Extraer t√≠tulo
                            titulo_elem = articulo.find('h2') or articulo.find('h3') or articulo.find('a')
                            titulo = titulo_elem.get_text(strip=True) if titulo_elem else 'Sin t√≠tulo'
                            
                            # Extraer enlace
                            link_elem = articulo.find('a', href=True)
                            link = link_elem['href'] if link_elem else ''
                            if link and not link.startswith('http'):
                                link = f"https://gestion.pe{link}"
                            
                            # Extraer descripci√≥n
                            desc_elem = articulo.find('p')
                            descripcion = desc_elem.get_text(strip=True) if desc_elem else ''
                            
                            if titulo and len(titulo) > 10:
                                noticias.append({
                                    'fecha': datetime.now(),
                                    'titulo': titulo,
                                    'descripcion': descripcion,
                                    'texto': f"{titulo} {descripcion}",
                                    'fuente': 'La Gesti√≥n (Web Scraping)',
                                    'url': link,
                                    'pais': 'Per√∫'
                                })
                        except Exception as e:
                            continue
                    
                    time.sleep(1)  # Respetar el servidor
                    
                except Exception as e:
                    print(f"    ‚ö†Ô∏è Error en {url}: {str(e)[:50]}")
                    continue
            
            print(f"    ‚úÖ {len(noticias)} noticias de La Gesti√≥n")
            
        except Exception as e:
            print(f"    ‚ùå Error general en La Gesti√≥n: {str(e)}")
        
        return noticias
    
    def scrape_larepublica(self, max_noticias=20):
        """
        Scrapear noticias de La Rep√∫blica (Per√∫)
        URL: https://larepublica.pe/economia/
        """
        noticias = []
        
        try:
            print("  üì∞ Scrapeando La Rep√∫blica...")
            
            urls = [
                'https://larepublica.pe/economia/',
                'https://larepublica.pe/tag/oro/'
            ]
            
            for url in urls[:1]:
                try:
                    response = requests.get(url, headers=self.headers, timeout=self.timeout)
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # Buscar art√≠culos
                    articulos = soup.find_all('article', limit=max_noticias)
                    
                    if not articulos:
                        articulos = soup.find_all('div', class_='news-item', limit=max_noticias)
                    
                    for articulo in articulos[:max_noticias]:
                        try:
                            titulo_elem = articulo.find('h2') or articulo.find('h3') or articulo.find('a')
                            titulo = titulo_elem.get_text(strip=True) if titulo_elem else ''
                            
                            link_elem = articulo.find('a', href=True)
                            link = link_elem['href'] if link_elem else ''
                            if link and not link.startswith('http'):
                                link = f"https://larepublica.pe{link}"
                            
                            desc_elem = articulo.find('p')
                            descripcion = desc_elem.get_text(strip=True) if desc_elem else ''
                            
                            if titulo and len(titulo) > 10:
                                noticias.append({
                                    'fecha': datetime.now(),
                                    'titulo': titulo,
                                    'descripcion': descripcion,
                                    'texto': f"{titulo} {descripcion}",
                                    'fuente': 'La Rep√∫blica (Web Scraping)',
                                    'url': link,
                                    'pais': 'Per√∫'
                                })
                        except:
                            continue
                    
                    time.sleep(1)
                    
                except Exception as e:
                    print(f"    ‚ö†Ô∏è Error en {url}: {str(e)[:50]}")
                    continue
            
            print(f"    ‚úÖ {len(noticias)} noticias de La Rep√∫blica")
            
        except Exception as e:
            print(f"    ‚ùå Error general en La Rep√∫blica: {str(e)}")
        
        return noticias
    
    def scrape_elcomercio(self, max_noticias=20):
        """
        Scrapear noticias de El Comercio (Per√∫)
        URL: https://elcomercio.pe/economia/
        """
        noticias = []
        
        try:
            print("  üì∞ Scrapeando El Comercio...")
            
            url = 'https://elcomercio.pe/economia/'
            
            try:
                response = requests.get(url, headers=self.headers, timeout=self.timeout)
                soup = BeautifulSoup(response.content, 'html.parser')
                
                articulos = soup.find_all('article', limit=max_noticias)
                
                for articulo in articulos[:max_noticias]:
                    try:
                        titulo_elem = articulo.find('h2') or articulo.find('h3') or articulo.find('a')
                        titulo = titulo_elem.get_text(strip=True) if titulo_elem else ''
                        
                        link_elem = articulo.find('a', href=True)
                        link = link_elem['href'] if link_elem else ''
                        if link and not link.startswith('http'):
                            link = f"https://elcomercio.pe{link}"
                        
                        if titulo and len(titulo) > 10:
                            noticias.append({
                                'fecha': datetime.now(),
                                'titulo': titulo,
                                'descripcion': '',
                                'texto': titulo,
                                'fuente': 'El Comercio (Web Scraping)',
                                'url': link,
                                'pais': 'Per√∫'
                            })
                    except:
                        continue
                
                time.sleep(1)
                
            except Exception as e:
                print(f"    ‚ö†Ô∏è Error: {str(e)[:50]}")
            
            print(f"    ‚úÖ {len(noticias)} noticias de El Comercio")
            
        except Exception as e:
            print(f"    ‚ùå Error general en El Comercio: {str(e)}")
        
        return noticias
    
    def scrape_kitco(self, max_noticias=15):
        """
        Scrapear Kitco.com - L√≠der mundial en noticias de oro
        URL: https://www.kitco.com/news/gold.html
        """
        noticias = []
        
        try:
            print("  üì∞ Scrapeando Kitco (Gold News)...")
            
            url = 'https://www.kitco.com/news/gold.html'
            
            try:
                response = requests.get(url, headers=self.headers, timeout=self.timeout)
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Kitco tiene una estructura espec√≠fica
                articulos = soup.find_all('div', class_='article', limit=max_noticias)
                
                if not articulos:
                    articulos = soup.find_all('article', limit=max_noticias)
                
                for articulo in articulos[:max_noticias]:
                    try:
                        titulo_elem = articulo.find('h3') or articulo.find('h2') or articulo.find('a')
                        titulo = titulo_elem.get_text(strip=True) if titulo_elem else ''
                        
                        link_elem = articulo.find('a', href=True)
                        link = link_elem['href'] if link_elem else ''
                        if link and not link.startswith('http'):
                            link = f"https://www.kitco.com{link}"
                        
                        desc_elem = articulo.find('p')
                        descripcion = desc_elem.get_text(strip=True) if desc_elem else ''
                        
                        if titulo and len(titulo) > 10:
                            noticias.append({
                                'fecha': datetime.now(),
                                'titulo': titulo,
                                'descripcion': descripcion,
                                'texto': f"{titulo} {descripcion}",
                                'fuente': 'Kitco (Web Scraping)',
                                'url': link,
                                'pais': 'Internacional'
                            })
                    except:
                        continue
                
                time.sleep(1)
                
            except Exception as e:
                print(f"    ‚ö†Ô∏è Error: {str(e)[:50]}")
            
            print(f"    ‚úÖ {len(noticias)} noticias de Kitco")
            
        except Exception as e:
            print(f"    ‚ùå Error general en Kitco: {str(e)}")
        
        return noticias
    
    def scrape_mining(self, max_noticias=15):
        """
        Scrapear Mining.com - Noticias de miner√≠a y oro
        URL: https://www.mining.com/category/gold/
        """
        noticias = []
        
        try:
            print("  üì∞ Scrapeando Mining.com...")
            
            url = 'https://www.mining.com/tag/gold/'
            
            try:
                response = requests.get(url, headers=self.headers, timeout=self.timeout)
                soup = BeautifulSoup(response.content, 'html.parser')
                
                articulos = soup.find_all('article', limit=max_noticias)
                
                for articulo in articulos[:max_noticias]:
                    try:
                        titulo_elem = articulo.find('h3') or articulo.find('h2')
                        titulo = titulo_elem.get_text(strip=True) if titulo_elem else ''
                        
                        link_elem = articulo.find('a', href=True)
                        link = link_elem['href'] if link_elem else ''
                        
                        if titulo and len(titulo) > 10:
                            noticias.append({
                                'fecha': datetime.now(),
                                'titulo': titulo,
                                'descripcion': '',
                                'texto': titulo,
                                'fuente': 'Mining.com (Web Scraping)',
                                'url': link,
                                'pais': 'Internacional'
                            })
                    except:
                        continue
                
                time.sleep(1)
                
            except Exception as e:
                print(f"    ‚ö†Ô∏è Error: {str(e)[:50]}")
            
            print(f"    ‚úÖ {len(noticias)} noticias de Mining.com")
            
        except Exception as e:
            print(f"    ‚ùå Error general en Mining.com: {str(e)}")
        
        return noticias
    
    def scrape_todas_las_fuentes(self, max_por_fuente=15):
        """
        Scrapear todas las fuentes disponibles
        
        Returns:
            DataFrame con todas las noticias scrapeadas
        """
        print("\nüåê Iniciando Web Scraping de m√∫ltiples fuentes...\n")
        
        todas_noticias = []
        
        # Scrapear cada fuente
        todas_noticias.extend(self.scrape_gestion(max_por_fuente))
        todas_noticias.extend(self.scrape_larepublica(max_por_fuente))
        todas_noticias.extend(self.scrape_elcomercio(max_por_fuente))
        todas_noticias.extend(self.scrape_kitco(max_por_fuente))
        todas_noticias.extend(self.scrape_mining(max_por_fuente))
        
        df = pd.DataFrame(todas_noticias)
        
        if not df.empty:
            # Eliminar duplicados por t√≠tulo
            df = df.drop_duplicates(subset=['titulo'], keep='first')
            print(f"\n‚úÖ Total: {len(df)} noticias √∫nicas scrapeadas")
            print(f"\nDistribuci√≥n por fuente:")
            print(df['fuente'].value_counts())
        else:
            print("\n‚ö†Ô∏è No se obtuvieron noticias del web scraping")
        
        return df

def obtener_noticias_scraping(max_por_fuente=15):
    """
    Funci√≥n simple para obtener noticias via web scraping
    Compatible con la estructura del dashboard
    """
    scraper = WebScraperNoticias()
    return scraper.scrape_todas_las_fuentes(max_por_fuente)

if __name__ == '__main__':
    print("="*60)
    print("PRUEBA DE WEB SCRAPING - Noticias de Oro y Econom√≠a")
    print("="*60)
    
    scraper = WebScraperNoticias()
    df_noticias = scraper.scrape_todas_las_fuentes(max_por_fuente=10)
    
    if not df_noticias.empty:
        print("\n" + "="*60)
        print("MUESTRA DE NOTICIAS SCRAPEADAS")
        print("="*60)
        print("\nPrimeras 5 noticias:")
        for idx, row in df_noticias.head(5).iterrows():
            print(f"\n{idx+1}. [{row['fuente']}]")
            print(f"   {row['titulo'][:80]}...")
            if row['url']:
                print(f"   URL: {row['url'][:60]}...")
        
        print(f"\n{'='*60}")
        print(f"‚úÖ Web scraping completado: {len(df_noticias)} noticias")
        print(f"{'='*60}")
    else:
        print("\n‚ùå No se obtuvieron noticias")
