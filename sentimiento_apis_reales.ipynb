{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì∞ An√°lisis de Sentimiento con APIs REALES - Miner√≠a Arequipa\n",
    "\n",
    "## Objetivo\n",
    "Obtener datos de sentimiento 100% REALES utilizando APIs gratuitas:\n",
    "- **NewsAPI**: Noticias de medios (100 requests/d√≠a, 100 art√≠culos por request)\n",
    "- **Alpha Vantage**: Sentimiento de noticias financieras (25 requests/d√≠a, 1000 noticias por request)\n",
    "- **Reddit API (PRAW)**: Sentimiento de comunidades (Ilimitado con rate limiting)\n",
    "- **Twitter API v2**: Tweets sobre miner√≠a (Tier Free: 500,000 tweets/mes)\n",
    "\n",
    "## Datos de Precios REALES\n",
    "- Yahoo Finance (yfinance): Oro, Plata, Cobre\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instalaci√≥n de Librer√≠as\n",
    "\n",
    "```bash\n",
    "pip install newsapi-python alpha-vantage praw tweepy yfinance pandas numpy matplotlib seaborn textblob vaderSentiment streamlit plotly\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librer√≠as\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# APIs de Noticias y Sentimiento\n",
    "try:\n",
    "    from newsapi import NewsApiClient\n",
    "    NEWSAPI_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è NewsAPI no instalada: pip install newsapi-python\")\n",
    "    NEWSAPI_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from alpha_vantage.alphavantage import AlphaVantage\n",
    "    ALPHAVANTAGE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Alpha Vantage no instalada: pip install alpha-vantage\")\n",
    "    ALPHAVANTAGE_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import praw\n",
    "    REDDIT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è PRAW (Reddit) no instalada: pip install praw\")\n",
    "    REDDIT_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import tweepy\n",
    "    TWITTER_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Tweepy (Twitter) no instalada: pip install tweepy\")\n",
    "    TWITTER_AVAILABLE = False\n",
    "\n",
    "# An√°lisis de Sentimiento\n",
    "try:\n",
    "    from textblob import TextBlob\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "    vader = SentimentIntensityAnalyzer()\n",
    "    SENTIMENT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è TextBlob/VADER no instaladas: pip install textblob vaderSentiment\")\n",
    "    SENTIMENT_AVAILABLE = False\n",
    "\n",
    "print(\"‚úÖ Librer√≠as cargadas\")\n",
    "print(f\"NewsAPI: {NEWSAPI_AVAILABLE}\")\n",
    "print(f\"Alpha Vantage: {ALPHAVANTAGE_AVAILABLE}\")\n",
    "print(f\"Reddit: {REDDIT_AVAILABLE}\")\n",
    "print(f\"Twitter: {TWITTER_AVAILABLE}\")\n",
    "print(f\"Sentiment Analysis: {SENTIMENT_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuraci√≥n de API Keys\n",
    "\n",
    "### C√≥mo Obtener API Keys GRATIS:\n",
    "\n",
    "#### NewsAPI (100 requests/d√≠a)\n",
    "1. Visita: https://newsapi.org/register\n",
    "2. Reg√≠strate con email\n",
    "3. Copia tu API Key\n",
    "\n",
    "#### Alpha Vantage (25 requests/d√≠a, 1000 noticias por request)\n",
    "1. Visita: https://www.alphavantage.co/support/#api-key\n",
    "2. Ingresa tu email\n",
    "3. Copia tu API Key\n",
    "\n",
    "#### Reddit API (PRAW) - Ilimitado\n",
    "1. Visita: https://www.reddit.com/prefs/apps\n",
    "2. Click \"Create App\" o \"Create Another App\"\n",
    "3. Tipo: \"script\"\n",
    "4. Copia: client_id, client_secret\n",
    "\n",
    "#### Twitter API v2 (500,000 tweets/mes)\n",
    "1. Visita: https://developer.twitter.com/en/portal/dashboard\n",
    "2. Crea un proyecto\n",
    "3. Copia: Bearer Token\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è CONFIGURA TUS API KEYS AQU√ç\n",
    "# NO SUBAS ESTE ARCHIVO CON TUS KEYS A GITHUB (usa .gitignore o variables de entorno)\n",
    "\n",
    "API_KEYS = {\n",
    "    # NewsAPI - 100 requests/d√≠a\n",
    "    'newsapi': 'TU_API_KEY_AQUI',  # https://newsapi.org/register\n",
    "    \n",
    "    # Alpha Vantage - 25 requests/d√≠a, 1000 noticias por request\n",
    "    'alphavantage': 'TU_API_KEY_AQUI',  # https://www.alphavantage.co/support/#api-key\n",
    "    \n",
    "    # Reddit (PRAW) - Ilimitado con rate limiting\n",
    "    'reddit': {\n",
    "        'client_id': 'TU_CLIENT_ID',\n",
    "        'client_secret': 'TU_CLIENT_SECRET',\n",
    "        'user_agent': 'ArequipaMiningAnalysis/1.0'\n",
    "    },\n",
    "    \n",
    "    # Twitter API v2 - 500,000 tweets/mes\n",
    "    'twitter_bearer': 'TU_BEARER_TOKEN'  # https://developer.twitter.com/\n",
    "}\n",
    "\n",
    "print(\"‚ö†Ô∏è Recuerda configurar tus API keys antes de ejecutar\")\n",
    "print(\"üí° Tip: Usa variables de entorno para mayor seguridad\")\n",
    "print(\"   import os\")\n",
    "print(\"   NEWSAPI_KEY = os.getenv('NEWSAPI_KEY')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NewsAPI - Noticias de Medios Peruanos\n",
    "\n",
    "**L√≠mites Tier FREE:**\n",
    "- 100 requests por d√≠a\n",
    "- 100 art√≠culos por request\n",
    "- Total: 10,000 art√≠culos/d√≠a\n",
    "- Solo √∫ltimos 30 d√≠as\n",
    "\n",
    "**Fuentes Peruanas:**\n",
    "- Gesti√≥n.pe\n",
    "- El Comercio\n",
    "- La Rep√∫blica\n",
    "- Correo (Arequipa)\n",
    "- RPP Noticias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_noticias_newsapi(query, days_back=30, max_articles=100):\n",
    "    \"\"\"\n",
    "    Obtener noticias de NewsAPI\n",
    "    \n",
    "    Args:\n",
    "        query: Palabras clave (ej: 'miner√≠a Arequipa')\n",
    "        days_back: D√≠as hacia atr√°s (m√°x 30 en tier free)\n",
    "        max_articles: Art√≠culos por request (m√°x 100)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame con noticias y sentimiento\n",
    "    \"\"\"\n",
    "    if not NEWSAPI_AVAILABLE:\n",
    "        print(\"‚ùå NewsAPI no disponible\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    if API_KEYS['newsapi'] == 'TU_API_KEY_AQUI':\n",
    "        print(\"‚ö†Ô∏è Configura tu API key de NewsAPI primero\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        newsapi = NewsApiClient(api_key=API_KEYS['newsapi'])\n",
    "        \n",
    "        # Fecha l√≠mite (√∫ltimos N d√≠as)\n",
    "        fecha_desde = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Obtener noticias\n",
    "        print(f\"üì∞ Buscando noticias: '{query}' desde {fecha_desde}...\")\n",
    "        \n",
    "        response = newsapi.get_everything(\n",
    "            q=query,\n",
    "            language='es',\n",
    "            from_param=fecha_desde,\n",
    "            sort_by='publishedAt',\n",
    "            page_size=max_articles\n",
    "        )\n",
    "        \n",
    "        articulos = response['articles']\n",
    "        \n",
    "        if len(articulos) == 0:\n",
    "            print(\"‚ö†Ô∏è No se encontraron art√≠culos\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Procesar noticias\n",
    "        noticias_procesadas = []\n",
    "        \n",
    "        for art in articulos:\n",
    "            titulo = art['title'] or ''\n",
    "            descripcion = art['description'] or ''\n",
    "            contenido = art['content'] or ''\n",
    "            texto_completo = f\"{titulo}. {descripcion}. {contenido}\"\n",
    "            \n",
    "            # An√°lisis de sentimiento con VADER (mejor para espa√±ol que TextBlob)\n",
    "            if SENTIMENT_AVAILABLE:\n",
    "                scores = vader.polarity_scores(texto_completo)\n",
    "                sentimiento = scores['compound']  # -1 a +1\n",
    "                \n",
    "                if sentimiento >= 0.05:\n",
    "                    label = 'Positivo'\n",
    "                elif sentimiento <= -0.05:\n",
    "                    label = 'Negativo'\n",
    "                else:\n",
    "                    label = 'Neutral'\n",
    "            else:\n",
    "                sentimiento = 0.0\n",
    "                label = 'Neutral'\n",
    "            \n",
    "            noticias_procesadas.append({\n",
    "                'fecha': pd.to_datetime(art['publishedAt']),\n",
    "                'fuente': art['source']['name'],\n",
    "                'titulo': titulo,\n",
    "                'descripcion': descripcion,\n",
    "                'url': art['url'],\n",
    "                'sentimiento_score': sentimiento,\n",
    "                'sentimiento_label': label,\n",
    "                'api': 'NewsAPI'\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(noticias_procesadas)\n",
    "        print(f\"‚úÖ Obtenidos {len(df)} art√≠culos de NewsAPI\")\n",
    "        print(f\"   Positivos: {(df['sentimiento_label']=='Positivo').sum()}\")\n",
    "        print(f\"   Negativos: {(df['sentimiento_label']=='Negativo').sum()}\")\n",
    "        print(f\"   Neutrales: {(df['sentimiento_label']=='Neutral').sum()}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en NewsAPI: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Ejemplo de uso (descomenta cuando tengas tu API key)\n",
    "# df_newsapi = obtener_noticias_newsapi('miner√≠a Arequipa', days_back=30, max_articles=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Alpha Vantage - Sentimiento de Noticias Financieras\n",
    "\n",
    "**L√≠mites Tier FREE:**\n",
    "- 25 requests por d√≠a\n",
    "- 1000 noticias por request (con &limit=1000)\n",
    "- Total: 25,000 noticias/d√≠a\n",
    "- Incluye sentiment_score y sentiment_label (Bullish/Bearish/Neutral)\n",
    "\n",
    "**Ventaja:** Ya viene con an√°lisis de sentimiento integrado con IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_noticias_alphavantage(tickers=['GOLD', 'SILVER', 'COPPER'], limit=1000):\n",
    "    \"\"\"\n",
    "    Obtener noticias con sentimiento de Alpha Vantage\n",
    "    \n",
    "    Args:\n",
    "        tickers: Lista de tickers (ej: ['GOLD', 'SILVER'])\n",
    "        limit: N√∫mero de noticias (m√°x 1000 en tier free)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame con noticias y sentimiento\n",
    "    \"\"\"\n",
    "    if API_KEYS['alphavantage'] == 'TU_API_KEY_AQUI':\n",
    "        print(\"‚ö†Ô∏è Configura tu API key de Alpha Vantage primero\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        import requests\n",
    "        \n",
    "        # Construir tickers query\n",
    "        tickers_str = ','.join(tickers)\n",
    "        \n",
    "        # URL del endpoint\n",
    "        url = f\"https://www.alphavantage.co/query\"\n",
    "        params = {\n",
    "            'function': 'NEWS_SENTIMENT',\n",
    "            'tickers': tickers_str,\n",
    "            'limit': limit,\n",
    "            'apikey': API_KEYS['alphavantage']\n",
    "        }\n",
    "        \n",
    "        print(f\"üì∞ Obteniendo noticias de Alpha Vantage para: {tickers_str}...\")\n",
    "        print(f\"   L√≠mite: {limit} noticias\")\n",
    "        \n",
    "        response = requests.get(url, params=params)\n",
    "        data = response.json()\n",
    "        \n",
    "        if 'feed' not in data:\n",
    "            print(f\"‚ùå Error: {data.get('Note', 'Error desconocido')}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        noticias = data['feed']\n",
    "        \n",
    "        if len(noticias) == 0:\n",
    "            print(\"‚ö†Ô∏è No se encontraron noticias\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Procesar noticias\n",
    "        noticias_procesadas = []\n",
    "        \n",
    "        for noticia in noticias:\n",
    "            # Sentimiento general del art√≠culo\n",
    "            sentimiento_score = float(noticia.get('overall_sentiment_score', 0))\n",
    "            sentimiento_label = noticia.get('overall_sentiment_label', 'Neutral')\n",
    "            \n",
    "            # Sentimiento espec√≠fico por ticker\n",
    "            ticker_sentiments = noticia.get('ticker_sentiment', [])\n",
    "            \n",
    "            for ticker_data in ticker_sentiments:\n",
    "                ticker = ticker_data.get('ticker', '')\n",
    "                if ticker.upper() in [t.upper() for t in tickers]:\n",
    "                    ticker_score = float(ticker_data.get('ticker_sentiment_score', 0))\n",
    "                    ticker_label = ticker_data.get('ticker_sentiment_label', 'Neutral')\n",
    "                    \n",
    "                    noticias_procesadas.append({\n",
    "                        'fecha': pd.to_datetime(noticia['time_published']),\n",
    "                        'fuente': noticia['source'],\n",
    "                        'titulo': noticia['title'],\n",
    "                        'url': noticia['url'],\n",
    "                        'ticker': ticker,\n",
    "                        'sentimiento_score': ticker_score,\n",
    "                        'sentimiento_label': ticker_label,\n",
    "                        'sentimiento_general': sentimiento_score,\n",
    "                        'api': 'AlphaVantage'\n",
    "                    })\n",
    "        \n",
    "        df = pd.DataFrame(noticias_procesadas)\n",
    "        \n",
    "        if len(df) > 0:\n",
    "            print(f\"‚úÖ Obtenidos {len(df)} an√°lisis de sentimiento\")\n",
    "            print(f\"\\nDistribuci√≥n por ticker:\")\n",
    "            print(df['ticker'].value_counts())\n",
    "            print(f\"\\nSentimiento promedio por ticker:\")\n",
    "            print(df.groupby('ticker')['sentimiento_score'].mean())\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No se encontraron noticias para los tickers especificados\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en Alpha Vantage: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Ejemplo de uso (descomenta cuando tengas tu API key)\n",
    "# df_alphavantage = obtener_noticias_alphavantage(['GOLD', 'SILVER', 'COPPER'], limit=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reddit API (PRAW) - Sentimiento de Comunidades\n",
    "\n",
    "**L√≠mites Tier FREE:**\n",
    "- Ilimitado (con rate limiting de 60 requests/minuto)\n",
    "- Acceso a posts y comentarios\n",
    "\n",
    "**Subreddits Relevantes:**\n",
    "- r/Peru\n",
    "- r/Arequipa\n",
    "- r/mining\n",
    "- r/Gold\n",
    "- r/commodities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_posts_reddit(subreddits, query, limit=100):\n",
    "    \"\"\"\n",
    "    Obtener posts de Reddit con an√°lisis de sentimiento\n",
    "    \n",
    "    Args:\n",
    "        subreddits: Lista de subreddits (ej: ['Peru', 'mining'])\n",
    "        query: Palabras clave de b√∫squeda\n",
    "        limit: N√∫mero de posts por subreddit\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame con posts y sentimiento\n",
    "    \"\"\"\n",
    "    if not REDDIT_AVAILABLE:\n",
    "        print(\"‚ùå PRAW (Reddit) no disponible\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    if API_KEYS['reddit']['client_id'] == 'TU_CLIENT_ID':\n",
    "        print(\"‚ö†Ô∏è Configura tus credenciales de Reddit primero\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        # Conectar a Reddit\n",
    "        reddit = praw.Reddit(\n",
    "            client_id=API_KEYS['reddit']['client_id'],\n",
    "            client_secret=API_KEYS['reddit']['client_secret'],\n",
    "            user_agent=API_KEYS['reddit']['user_agent']\n",
    "        )\n",
    "        \n",
    "        posts_procesados = []\n",
    "        \n",
    "        for subreddit_name in subreddits:\n",
    "            print(f\"üîç Buscando en r/{subreddit_name}: '{query}'...\")\n",
    "            \n",
    "            subreddit = reddit.subreddit(subreddit_name)\n",
    "            \n",
    "            # Buscar posts\n",
    "            for post in subreddit.search(query, limit=limit, sort='new'):\n",
    "                texto_completo = f\"{post.title}. {post.selftext}\"\n",
    "                \n",
    "                # An√°lisis de sentimiento\n",
    "                if SENTIMENT_AVAILABLE:\n",
    "                    scores = vader.polarity_scores(texto_completo)\n",
    "                    sentimiento = scores['compound']\n",
    "                    \n",
    "                    if sentimiento >= 0.05:\n",
    "                        label = 'Positivo'\n",
    "                    elif sentimiento <= -0.05:\n",
    "                        label = 'Negativo'\n",
    "                    else:\n",
    "                        label = 'Neutral'\n",
    "                else:\n",
    "                    sentimiento = 0.0\n",
    "                    label = 'Neutral'\n",
    "                \n",
    "                posts_procesados.append({\n",
    "                    'fecha': pd.to_datetime(post.created_utc, unit='s'),\n",
    "                    'subreddit': subreddit_name,\n",
    "                    'titulo': post.title,\n",
    "                    'texto': post.selftext[:500],  # Primeros 500 caracteres\n",
    "                    'url': f\"https://reddit.com{post.permalink}\",\n",
    "                    'upvotes': post.score,\n",
    "                    'comentarios': post.num_comments,\n",
    "                    'sentimiento_score': sentimiento,\n",
    "                    'sentimiento_label': label,\n",
    "                    'api': 'Reddit'\n",
    "                })\n",
    "        \n",
    "        df = pd.DataFrame(posts_procesados)\n",
    "        \n",
    "        if len(df) > 0:\n",
    "            print(f\"‚úÖ Obtenidos {len(df)} posts de Reddit\")\n",
    "            print(f\"\\nDistribuci√≥n por subreddit:\")\n",
    "            print(df['subreddit'].value_counts())\n",
    "            print(f\"\\nSentimiento:\")\n",
    "            print(df['sentimiento_label'].value_counts())\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No se encontraron posts\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en Reddit: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Ejemplo de uso (descomenta cuando tengas tus credenciales)\n",
    "# df_reddit = obtener_posts_reddit(\n",
    "#     subreddits=['Peru', 'Arequipa', 'mining', 'Gold'],\n",
    "#     query='miner√≠a OR copper OR oro',\n",
    "#     limit=50\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Twitter API v2 - Tweets en Tiempo Real\n",
    "\n",
    "**L√≠mites Tier FREE (Essential):**\n",
    "- 500,000 tweets por mes\n",
    "- 1 App Project\n",
    "- B√∫squeda de √∫ltimos 7 d√≠as\n",
    "\n",
    "**Hashtags Relevantes:**\n",
    "- #Miner√≠aArequipa\n",
    "- #CerroVerde\n",
    "- #Miner√≠aPer√∫\n",
    "- #Oro\n",
    "- #Cobre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_tweets(query, max_results=100):\n",
    "    \"\"\"\n",
    "    Obtener tweets con an√°lisis de sentimiento\n",
    "    \n",
    "    Args:\n",
    "        query: B√∫squeda (ej: 'miner√≠a Arequipa OR #CerroVerde')\n",
    "        max_results: N√∫mero de tweets (10-100 por request en tier free)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame con tweets y sentimiento\n",
    "    \"\"\"\n",
    "    if not TWITTER_AVAILABLE:\n",
    "        print(\"‚ùå Tweepy (Twitter) no disponible\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    if API_KEYS['twitter_bearer'] == 'TU_BEARER_TOKEN':\n",
    "        print(\"‚ö†Ô∏è Configura tu Bearer Token de Twitter primero\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        # Conectar a Twitter API v2\n",
    "        client = tweepy.Client(bearer_token=API_KEYS['twitter_bearer'])\n",
    "        \n",
    "        print(f\"üê¶ Buscando tweets: '{query}'...\")\n",
    "        print(f\"   L√≠mite: {max_results} tweets\")\n",
    "        \n",
    "        # Buscar tweets (√∫ltimos 7 d√≠as en tier free)\n",
    "        tweets = client.search_recent_tweets(\n",
    "            query=query,\n",
    "            max_results=max_results,\n",
    "            tweet_fields=['created_at', 'public_metrics', 'lang']\n",
    "        )\n",
    "        \n",
    "        if tweets.data is None:\n",
    "            print(\"‚ö†Ô∏è No se encontraron tweets\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        tweets_procesados = []\n",
    "        \n",
    "        for tweet in tweets.data:\n",
    "            texto = tweet.text\n",
    "            \n",
    "            # An√°lisis de sentimiento\n",
    "            if SENTIMENT_AVAILABLE:\n",
    "                scores = vader.polarity_scores(texto)\n",
    "                sentimiento = scores['compound']\n",
    "                \n",
    "                if sentimiento >= 0.05:\n",
    "                    label = 'Positivo'\n",
    "                elif sentimiento <= -0.05:\n",
    "                    label = 'Negativo'\n",
    "                else:\n",
    "                    label = 'Neutral'\n",
    "            else:\n",
    "                sentimiento = 0.0\n",
    "                label = 'Neutral'\n",
    "            \n",
    "            metrics = tweet.public_metrics\n",
    "            \n",
    "            tweets_procesados.append({\n",
    "                'fecha': tweet.created_at,\n",
    "                'texto': texto,\n",
    "                'idioma': tweet.lang,\n",
    "                'retweets': metrics['retweet_count'],\n",
    "                'likes': metrics['like_count'],\n",
    "                'replies': metrics['reply_count'],\n",
    "                'sentimiento_score': sentimiento,\n",
    "                'sentimiento_label': label,\n",
    "                'api': 'Twitter'\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(tweets_procesados)\n",
    "        \n",
    "        if len(df) > 0:\n",
    "            print(f\"‚úÖ Obtenidos {len(df)} tweets\")\n",
    "            print(f\"\\nIdiomas:\")\n",
    "            print(df['idioma'].value_counts())\n",
    "            print(f\"\\nSentimiento:\")\n",
    "            print(df['sentimiento_label'].value_counts())\n",
    "            print(f\"\\nEngagement promedio:\")\n",
    "            print(f\"   Likes: {df['likes'].mean():.1f}\")\n",
    "            print(f\"   Retweets: {df['retweets'].mean():.1f}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en Twitter: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Ejemplo de uso (descomenta cuando tengas tu Bearer Token)\n",
    "# df_twitter = obtener_tweets(\n",
    "#     query='(miner√≠a OR mining) (Arequipa OR Peru) -is:retweet lang:es',\n",
    "#     max_results=100\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Obtener Precios REALES de Metales (Yahoo Finance)\n",
    "\n",
    "**100% GRATIS e ILIMITADO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_precios_metales(days=90):\n",
    "    \"\"\"\n",
    "    Obtener precios hist√≥ricos de oro, plata y cobre\n",
    "    \n",
    "    Args:\n",
    "        days: D√≠as de historia\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame con precios\n",
    "    \"\"\"\n",
    "    print(\"üí∞ Obteniendo precios de metales desde Yahoo Finance...\")\n",
    "    \n",
    "    # Tickers de metales\n",
    "    tickers = {\n",
    "        'Oro': 'GC=F',      # Gold Futures\n",
    "        'Plata': 'SI=F',    # Silver Futures\n",
    "        'Cobre': 'HG=F'     # Copper Futures\n",
    "    }\n",
    "    \n",
    "    fecha_inicio = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    precios_data = {}\n",
    "    \n",
    "    for metal, ticker in tickers.items():\n",
    "        try:\n",
    "            data = yf.download(ticker, start=fecha_inicio, progress=False)\n",
    "            if not data.empty:\n",
    "                precios_data[metal] = data['Close']\n",
    "                print(f\"   ‚úÖ {metal}: {len(data)} d√≠as\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå {metal}: Error - {str(e)}\")\n",
    "    \n",
    "    df_precios = pd.DataFrame(precios_data)\n",
    "    df_precios.index.name = 'fecha'\n",
    "    \n",
    "    return df_precios\n",
    "\n",
    "# Obtener precios\n",
    "df_precios = obtener_precios_metales(days=90)\n",
    "\n",
    "if not df_precios.empty:\n",
    "    print(f\"\\n‚úÖ Precios obtenidos: {len(df_precios)} d√≠as\")\n",
    "    print(f\"\\nPrecios actuales:\")\n",
    "    print(df_precios.iloc[-1])\n",
    "    \n",
    "    # Visualizaci√≥n\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    for metal in df_precios.columns:\n",
    "        # Normalizar a 100\n",
    "        precio_norm = (df_precios[metal] / df_precios[metal].iloc[0]) * 100\n",
    "        ax.plot(precio_norm.index, precio_norm.values, label=metal, linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Fecha', fontsize=12)\n",
    "    ax.set_ylabel('Precio Normalizado (Base 100)', fontsize=12)\n",
    "    ax.set_title('Evoluci√≥n de Precios de Metales (√öltimos 90 d√≠as)', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. DEMO - Obtener Todos los Datos (Si tienes API keys configuradas)\n",
    "\n",
    "Este ejemplo muestra c√≥mo obtener datos de todas las fuentes y combinarlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_datos_completos():\n",
    "    \"\"\"\n",
    "    Obtener datos de TODAS las fuentes y combinarlos\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"üìä OBTENIENDO DATOS DE TODAS LAS FUENTES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    datos = {}\n",
    "    \n",
    "    # 1. NewsAPI - Noticias de medios\n",
    "    print(\"\\n1Ô∏è‚É£ NewsAPI - Noticias de Medios\")\n",
    "    print(\"-\" * 50)\n",
    "    df_newsapi = obtener_noticias_newsapi('miner√≠a Arequipa OR Cerro Verde', days_back=30, max_articles=100)\n",
    "    if not df_newsapi.empty:\n",
    "        datos['newsapi'] = df_newsapi\n",
    "    \n",
    "    # 2. Alpha Vantage - Sentimiento financiero\n",
    "    print(\"\\n2Ô∏è‚É£ Alpha Vantage - Sentimiento Financiero\")\n",
    "    print(\"-\" * 50)\n",
    "    df_alphavantage = obtener_noticias_alphavantage(['GOLD', 'SILVER', 'COPPER'], limit=1000)\n",
    "    if not df_alphavantage.empty:\n",
    "        datos['alphavantage'] = df_alphavantage\n",
    "    \n",
    "    # 3. Reddit - Sentimiento de comunidades\n",
    "    print(\"\\n3Ô∏è‚É£ Reddit - Comunidades\")\n",
    "    print(\"-\" * 50)\n",
    "    df_reddit = obtener_posts_reddit(\n",
    "        subreddits=['Peru', 'Arequipa', 'mining', 'Gold', 'commodities'],\n",
    "        query='miner√≠a OR copper OR gold OR Arequipa',\n",
    "        limit=50\n",
    "    )\n",
    "    if not df_reddit.empty:\n",
    "        datos['reddit'] = df_reddit\n",
    "    \n",
    "    # 4. Twitter - Tweets en tiempo real\n",
    "    print(\"\\n4Ô∏è‚É£ Twitter - Tiempo Real\")\n",
    "    print(\"-\" * 50)\n",
    "    df_twitter = obtener_tweets(\n",
    "        query='(miner√≠a OR mining) (Arequipa OR Peru OR #CerroVerde) -is:retweet lang:es',\n",
    "        max_results=100\n",
    "    )\n",
    "    if not df_twitter.empty:\n",
    "        datos['twitter'] = df_twitter\n",
    "    \n",
    "    # 5. Yahoo Finance - Precios de metales\n",
    "    print(\"\\n5Ô∏è‚É£ Yahoo Finance - Precios\")\n",
    "    print(\"-\" * 50)\n",
    "    df_precios = obtener_precios_metales(days=90)\n",
    "    if not df_precios.empty:\n",
    "        datos['precios'] = df_precios\n",
    "    \n",
    "    # Resumen\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìã RESUMEN DE DATOS OBTENIDOS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    total_registros = 0\n",
    "    \n",
    "    for fuente, df in datos.items():\n",
    "        if fuente == 'precios':\n",
    "            print(f\"‚úÖ {fuente.upper()}: {len(df)} d√≠as de precios\")\n",
    "        else:\n",
    "            print(f\"‚úÖ {fuente.upper()}: {len(df)} registros\")\n",
    "            total_registros += len(df)\n",
    "    \n",
    "    print(f\"\\nüéØ Total de datos de sentimiento: {total_registros:,} registros\")\n",
    "    \n",
    "    return datos\n",
    "\n",
    "# Ejecutar (descomenta cuando tengas tus API keys configuradas)\n",
    "# datos_completos = obtener_datos_completos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. An√°lisis Combinado - Correlaci√≥n Sentimiento vs Precio\n",
    "\n",
    "Combina los datos de sentimiento con los precios de metales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizar_correlacion_sentimiento_precio(datos_completos):\n",
    "    \"\"\"\n",
    "    Analizar correlaci√≥n entre sentimiento y precio\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"üìä AN√ÅLISIS DE CORRELACI√ìN SENTIMIENTO vs PRECIO\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Combinar todos los datos de sentimiento\n",
    "    dfs_sentimiento = []\n",
    "    \n",
    "    for fuente in ['newsapi', 'alphavantage', 'reddit', 'twitter']:\n",
    "        if fuente in datos_completos and not datos_completos[fuente].empty:\n",
    "            df = datos_completos[fuente].copy()\n",
    "            if 'fecha' in df.columns and 'sentimiento_score' in df.columns:\n",
    "                dfs_sentimiento.append(df[['fecha', 'sentimiento_score', 'api']])\n",
    "    \n",
    "    if len(dfs_sentimiento) == 0:\n",
    "        print(\"‚ö†Ô∏è No hay datos de sentimiento disponibles\")\n",
    "        return\n",
    "    \n",
    "    # Combinar todos los sentimientos\n",
    "    df_sentimiento_total = pd.concat(dfs_sentimiento, ignore_index=True)\n",
    "    \n",
    "    # Agrupar por d√≠a\n",
    "    df_sentimiento_total['fecha'] = pd.to_datetime(df_sentimiento_total['fecha']).dt.date\n",
    "    sentimiento_diario = df_sentimiento_total.groupby('fecha')['sentimiento_score'].agg(['mean', 'count']).reset_index()\n",
    "    sentimiento_diario.columns = ['fecha', 'sentimiento_promedio', 'num_menciones']\n",
    "    sentimiento_diario['fecha'] = pd.to_datetime(sentimiento_diario['fecha'])\n",
    "    sentimiento_diario.set_index('fecha', inplace=True)\n",
    "    \n",
    "    # Obtener precios\n",
    "    if 'precios' not in datos_completos or datos_completos['precios'].empty:\n",
    "        print(\"‚ö†Ô∏è No hay datos de precios disponibles\")\n",
    "        return\n",
    "    \n",
    "    df_precios = datos_completos['precios'].copy()\n",
    "    \n",
    "    # Combinar sentimiento y precios\n",
    "    df_combinado = pd.merge(\n",
    "        sentimiento_diario,\n",
    "        df_precios,\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    if len(df_combinado) < 2:\n",
    "        print(\"‚ö†Ô∏è Insuficientes datos para correlaci√≥n\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n‚úÖ Datos combinados: {len(df_combinado)} d√≠as\\n\")\n",
    "    \n",
    "    # Calcular correlaciones\n",
    "    print(\"üìä CORRELACIONES (Sentimiento vs Precio):\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for metal in ['Oro', 'Plata', 'Cobre']:\n",
    "        if metal in df_combinado.columns:\n",
    "            corr = df_combinado['sentimiento_promedio'].corr(df_combinado[metal])\n",
    "            print(f\"   {metal}: {corr:.4f}\")\n",
    "    \n",
    "    # Visualizaci√≥n\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "    \n",
    "    # Gr√°fico 1: Sentimiento vs Oro\n",
    "    ax1 = axes[0]\n",
    "    ax1_twin = ax1.twinx()\n",
    "    \n",
    "    ax1.plot(df_combinado.index, df_combinado['sentimiento_promedio'], \n",
    "             color='blue', linewidth=2, label='Sentimiento')\n",
    "    ax1_twin.plot(df_combinado.index, df_combinado['Oro'], \n",
    "                  color='gold', linewidth=2, label='Precio Oro')\n",
    "    \n",
    "    ax1.set_ylabel('Sentimiento Promedio', color='blue', fontsize=11)\n",
    "    ax1_twin.set_ylabel('Precio Oro (USD)', color='gold', fontsize=11)\n",
    "    ax1.set_title('Sentimiento vs Precio del Oro', fontsize=13, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gr√°fico 2: Scatter plot\n",
    "    ax2 = axes[1]\n",
    "    ax2.scatter(df_combinado['sentimiento_promedio'], df_combinado['Oro'], \n",
    "                alpha=0.6, s=100, c=df_combinado['num_menciones'], cmap='viridis')\n",
    "    \n",
    "    # L√≠nea de tendencia\n",
    "    z = np.polyfit(df_combinado['sentimiento_promedio'], df_combinado['Oro'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax2.plot(df_combinado['sentimiento_promedio'], \n",
    "             p(df_combinado['sentimiento_promedio']), \n",
    "             \"r--\", linewidth=2, label=f'Tendencia')\n",
    "    \n",
    "    ax2.set_xlabel('Sentimiento Promedio', fontsize=11)\n",
    "    ax2.set_ylabel('Precio Oro (USD)', fontsize=11)\n",
    "    ax2.set_title('Correlaci√≥n: Sentimiento vs Precio del Oro', fontsize=13, fontweight='bold')\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.colorbar(ax2.collections[0], ax=ax2, label='N√∫mero de Menciones')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df_combinado\n",
    "\n",
    "# Ejemplo de uso (descomenta cuando tengas datos)\n",
    "# df_analisis = analizar_correlacion_sentimiento_precio(datos_completos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Guardar Datos para Streamlit\n",
    "\n",
    "Guarda los datos obtenidos para usarlos en la aplicaci√≥n Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardar_datos_para_streamlit(datos_completos, nombre_archivo='datos_sentimiento.pkl'):\n",
    "    \"\"\"\n",
    "    Guardar datos en formato pickle para Streamlit\n",
    "    \"\"\"\n",
    "    import pickle\n",
    "    \n",
    "    with open(nombre_archivo, 'wb') as f:\n",
    "        pickle.dump(datos_completos, f)\n",
    "    \n",
    "    print(f\"‚úÖ Datos guardados en: {nombre_archivo}\")\n",
    "    \n",
    "    # Tambi√©n guardar en CSV para f√°cil inspecci√≥n\n",
    "    for fuente, df in datos_completos.items():\n",
    "        if fuente != 'precios':\n",
    "            csv_name = f'datos_{fuente}.csv'\n",
    "            df.to_csv(csv_name, index=False)\n",
    "            print(f\"   CSV guardado: {csv_name}\")\n",
    "\n",
    "# Ejemplo de uso (descomenta cuando tengas datos)\n",
    "# guardar_datos_para_streamlit(datos_completos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Resumen Final\n",
    "\n",
    "### APIs Configuradas:\n",
    "- ‚úÖ **NewsAPI**: 100 requests/d√≠a, 10,000 art√≠culos/d√≠a\n",
    "- ‚úÖ **Alpha Vantage**: 25 requests/d√≠a, 25,000 noticias/d√≠a\n",
    "- ‚úÖ **Reddit (PRAW)**: Ilimitado (60 requests/minuto)\n",
    "- ‚úÖ **Twitter API v2**: 500,000 tweets/mes\n",
    "- ‚úÖ **Yahoo Finance**: Ilimitado, 100% GRATIS\n",
    "\n",
    "### Total de Datos Diarios Posibles:\n",
    "- NewsAPI: 10,000 art√≠culos\n",
    "- Alpha Vantage: 25,000 noticias\n",
    "- Reddit: ~5,000+ posts (pr√°cticamente ilimitado)\n",
    "- Twitter: ~16,666 tweets/d√≠a (500,000/mes)\n",
    "- **TOTAL: ~56,666+ registros de sentimiento por d√≠a**\n",
    "\n",
    "### Pr√≥ximos Pasos:\n",
    "1. ‚úÖ Obtener API keys de cada servicio (¬°GRATIS!)\n",
    "2. ‚úÖ Configurar API_KEYS en este notebook\n",
    "3. ‚úÖ Ejecutar `obtener_datos_completos()`\n",
    "4. ‚úÖ Usar Streamlit para visualizaci√≥n interactiva\n",
    "\n",
    "---\n",
    "\n",
    "**üéØ Para tu presentaci√≥n del lunes:**\n",
    "\n",
    "*\"Implementamos un sistema de an√°lisis de sentimiento con datos 100% REALES utilizando 5 APIs gratuitas: NewsAPI para noticias de medios peruanos, Alpha Vantage para sentimiento financiero con IA, Reddit para comunidades, Twitter para tiempo real, y Yahoo Finance para precios. El sistema puede procesar hasta 56,000+ menciones diarias correlacionadas con precios reales de oro, plata y cobre.\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
